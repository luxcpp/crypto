---
title: CUDA Backend
description: GPU acceleration via NVIDIA CUDA compute kernels
---

# CUDA Backend

The CUDA backend provides GPU acceleration for cryptographic operations on NVIDIA GPUs, leveraging CUDA compute kernels for high-throughput parallel processing.

## Requirements

- **CUDA Toolkit 12.0+** (CUDA 12.x recommended)
- **NVIDIA GPU** with Compute Capability 7.0+ (Volta, Turing, Ampere, Ada, Hopper)
- **Linux or Windows** with NVIDIA driver 525.0+
- **cuBLAS** and **cuDNN** (included in CUDA Toolkit)

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    C++ API Layer                             │
│              (bls.cpp, mldsa.cpp, hash.cpp)                  │
├─────────────────────────────────────────────────────────────┤
│                  CUDA C++ Wrapper                            │
│           (cuda_bls.cu, cuda_hash.cu, etc.)                  │
├─────────────────────────────────────────────────────────────┤
│                  CUDA Compute Kernels                        │
│              (bls.cu, hash.cu, ntt.cu)                       │
├─────────────────────────────────────────────────────────────┤
│                   CUDA Runtime/Driver                        │
│            (Streams, Events, Memory Pools)                   │
├─────────────────────────────────────────────────────────────┤
│                  NVIDIA GPU Hardware                         │
│          (RTX 4090, A100, H100, etc.)                        │
└─────────────────────────────────────────────────────────────┘
```

## Kernel Implementation

### Field Arithmetic

Montgomery multiplication for BLS12-381:

```cuda
// 384-bit Montgomery multiplication on CUDA
__global__ void mont_mul_384(
    const uint32_t* __restrict__ a,
    const uint32_t* __restrict__ b,
    uint32_t* __restrict__ result,
    const uint32_t* __restrict__ modulus,
    const uint32_t inv,
    int count
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid >= count) return;

    uint32_t t[24];  // 768-bit intermediate

    // Schoolbook multiplication
    #pragma unroll
    for (int i = 0; i < 12; i++) {
        uint64_t carry = 0;
        #pragma unroll
        for (int j = 0; j < 12; j++) {
            uint64_t prod = (uint64_t)a[tid * 12 + i] * b[j]
                          + t[i + j] + carry;
            t[i + j] = (uint32_t)prod;
            carry = prod >> 32;
        }
        t[i + 12] = (uint32_t)carry;
    }

    // Montgomery reduction
    mont_reduce(t, modulus, inv);

    // Store result
    #pragma unroll
    for (int i = 0; i < 12; i++) {
        result[tid * 12 + i] = t[i];
    }
}
```

### Multi-Scalar Multiplication (MSM)

Pippenger's algorithm optimized for CUDA:

```cuda
// Bucket accumulation for MSM
__global__ void msm_bucket_accumulate(
    const Point* __restrict__ points,
    const Scalar* __restrict__ scalars,
    Point* __restrict__ buckets,
    int window_size,
    int window_idx,
    int count
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid >= count) return;

    // Extract window from scalar
    uint32_t window = extract_window(scalars[tid], window_idx, window_size);

    if (window != 0) {
        // Atomic add to bucket
        atomicPointAdd(&buckets[window], points[tid]);
    }
}

// Bucket reduction
__global__ void msm_bucket_reduce(
    Point* __restrict__ buckets,
    Point* __restrict__ result,
    int num_buckets,
    int num_windows
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid >= num_windows) return;

    // Running sum reduction
    Point running_sum = point_identity();
    Point total = point_identity();

    for (int i = num_buckets - 1; i >= 0; i--) {
        running_sum = point_add(running_sum, buckets[tid * num_buckets + i]);
        total = point_add(total, running_sum);
    }

    result[tid] = total;
}
```

### Number Theoretic Transform (NTT)

Radix-2 NTT for polynomial operations:

```cuda
// NTT butterfly operation
__global__ void ntt_butterfly(
    int32_t* __restrict__ poly,
    const int32_t* __restrict__ twiddles,
    int layer,
    int32_t modulus,
    int n
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;

    int step = 1 << layer;
    int block = tid / step;
    int j = tid % step;
    int i = block * (step << 1) + j;

    if (i + step >= n) return;

    int32_t u = poly[i];
    int32_t v = mont_mul(twiddles[step + j], poly[i + step], modulus);

    poly[i] = mod_add(u, v, modulus);
    poly[i + step] = mod_sub(u, v, modulus);
}
```

### BLAKE3 Parallel Hashing

```cuda
// BLAKE3 chunk processing
__global__ void blake3_chunks(
    const uint8_t* __restrict__ input,
    uint32_t* __restrict__ chunk_outputs,
    uint32_t input_len,
    int num_chunks
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid >= num_chunks) return;

    uint32_t chunk_start = tid * BLAKE3_CHUNK_LEN;
    if (chunk_start >= input_len) return;

    uint32_t chunk_len = min(BLAKE3_CHUNK_LEN, input_len - chunk_start);

    // Initialize chunk state
    uint32_t cv[8] = {
        IV[0], IV[1], IV[2], IV[3],
        IV[4], IV[5], IV[6], IV[7]
    };

    // Process blocks in chunk
    uint32_t block_count = (chunk_len + 63) / 64;
    for (uint32_t b = 0; b < block_count; b++) {
        uint32_t block[16];
        load_block(block, input + chunk_start + b * 64);

        uint32_t flags = (b == 0 ? CHUNK_START : 0)
                       | (b == block_count - 1 ? CHUNK_END : 0);
        compress(cv, block, tid, b, flags);
    }

    // Store chunk output
    #pragma unroll
    for (int i = 0; i < 8; i++) {
        chunk_outputs[tid * 8 + i] = cv[i];
    }
}
```

## Memory Management

### Device Memory Allocation

```cpp
// Create CUDA buffer from host data
void* create_buffer(const void* data, size_t size) {
    void* d_ptr;
    cudaMalloc(&d_ptr, size);
    cudaMemcpy(d_ptr, data, size, cudaMemcpyHostToDevice);
    return d_ptr;
}
```

### Unified Memory (Pascal+)

```cpp
// Managed memory for automatic migration
void* d_ptr;
cudaMallocManaged(&d_ptr, size);

// Prefetch to device for better performance
cudaMemPrefetchAsync(d_ptr, size, device_id, stream);
```

### Asynchronous Streams

```cpp
// Create stream for async operations
cudaStream_t stream;
cudaStreamCreate(&stream);

// Async memory transfer and kernel execution
cudaMemcpyAsync(d_input, h_input, size, cudaMemcpyHostToDevice, stream);
kernel<<<grid, block, 0, stream>>>(d_input, d_output);
cudaMemcpyAsync(h_output, d_output, size, cudaMemcpyDeviceToHost, stream);

// Synchronize when done
cudaStreamSynchronize(stream);
```

## Configuration

### Device Selection

```cpp
// Get device count
int device_count;
cudaGetDeviceCount(&device_count);

// List all available devices
for (int i = 0; i < device_count; i++) {
    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, i);
    printf("Device %d: %s (Compute %d.%d)\n",
           i, prop.name, prop.major, prop.minor);
}

// Select specific device
cudaSetDevice(device_id);
```

### Compile Options

```cmake
# Enable CUDA backend
set(LUX_CRYPTO_CUDA ON)

# Specify compute capability
set(CMAKE_CUDA_ARCHITECTURES "70;75;80;86;89;90")

# Optimization flags
set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -O3 --use_fast_math")
```

## Performance Tuning

### Thread Configuration

```cpp
// Optimal thread block size (typically 256 for compute-bound)
dim3 blockSize(256, 1, 1);

// Calculate grid size
int numElements = ...;
int gridWidth = (numElements + 255) / 256;
dim3 gridSize(gridWidth, 1, 1);

// Launch kernel
kernel<<<gridSize, blockSize>>>(args);
```

### Occupancy Optimization

```cpp
// Query optimal block size for kernel
int minGridSize, optBlockSize;
cudaOccupancyMaxPotentialBlockSize(&minGridSize, &optBlockSize, kernel);

// Launch with optimal configuration
kernel<<<minGridSize, optBlockSize>>>(args);
```

### Concurrent Execution

```cpp
// Create multiple streams for concurrent operations
std::vector<cudaStream_t> streams(num_streams);
for (auto& s : streams) {
    cudaStreamCreate(&s);
}

// Launch concurrent operations
for (int i = 0; i < num_operations; i++) {
    kernel<<<grid, block, 0, streams[i % num_streams]>>>(args);
}

// Synchronize all streams
for (auto& s : streams) {
    cudaStreamSynchronize(s);
}
```

## Error Handling

```cpp
// Check for CUDA errors
#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA error: " << cudaGetErrorString(err) \
                      << " at " << __FILE__ << ":" << __LINE__ << std::endl; \
            exit(EXIT_FAILURE); \
        } \
    } while(0)

// Usage
CUDA_CHECK(cudaMalloc(&d_ptr, size));
CUDA_CHECK(cudaMemcpy(d_ptr, h_ptr, size, cudaMemcpyHostToDevice));
```

## Limitations

1. **Platform**: Linux and Windows only (no macOS CUDA support)
2. **GPU Memory**: Limited by device VRAM (use streaming for large datasets)
3. **Driver Dependency**: Requires matching NVIDIA driver version
4. **Kernel Launch Overhead**: Small operations may be faster on CPU

## Benchmarks

Detailed benchmarks on NVIDIA RTX 4090 (16384 CUDA cores):

| Operation | Time | Throughput |
|-----------|------|------------|
| BLS Sign (batch 1000) | 45ms | 22,222/s |
| BLS Verify (batch 1000) | 85ms | 11,765/s |
| MSM 2^16 points | 18ms | 55/s |
| BLAKE3 1GB | 65ms | 15.4 GB/s |
| NTT 2^20 | 6ms | 166/s |
| ML-DSA-65 Sign | 32μs | 31,250/s |

### GPU Comparison

| GPU | BLS Sign/s | MSM 2^16 | BLAKE3 GB/s |
|-----|------------|----------|-------------|
| RTX 4090 | 22,222 | 18ms | 15.4 |
| RTX 3090 | 15,000 | 28ms | 10.2 |
| A100 (80GB) | 25,000 | 15ms | 18.5 |
| H100 | 35,000 | 10ms | 25.0 |

## Use Cases

The CUDA backend is preferred for:

1. **High-Throughput Servers**: Data center deployments with NVIDIA GPUs
2. **Large Batch Operations**: Maximizes GPU utilization
3. **MSM Operations**: Exceptional performance for ZK-proof generation
4. **ML Workloads**: Combine crypto with ML on same hardware

## Building with CUDA

```bash
# Configure with CUDA support
cmake -DLUX_CRYPTO_CUDA=ON \
      -DCMAKE_CUDA_ARCHITECTURES="80;86;89;90" \
      -B build

# Build
cmake --build build

# Test CUDA backend
./build/tests/cuda_tests
```

## Fallback Behavior

When CUDA is unavailable, the library automatically uses CPU:

```cpp
// This works on any platform
auto sig = lux::crypto::bls_sign(sk, message);
// Uses CUDA if available, otherwise CPU
```
